

# ğŸ¦ Qameleon â€” Question Answering with Transformers

This project demonstrates how to use the ğŸ¤— **Transformers** library to build a simple Question Answering (QA) pipeline powered by a pretrained **DistilBERT** model fine-tuned on SQuAD.

It asks a question about a text context (here: *What is LangChain?*), extracts an answer, and reports the confidence.

---

## ğŸ“„ Example Output

```
Question: What is LangChain?
QA extracted answer: LangChain is an open-source framework for developing applications powered by language models (confidence: 0.87)
```

---

## ğŸš€ How It Works

âœ… Loads a **question-answering pipeline** from Hugging Face ğŸ¤—
âœ… Provides a text *context* and a *question*
âœ… The LLM extracts the most relevant answer span from the context
âœ… Reports the answer and the model's confidence score

---

## ğŸ§° Requirements

* Python â‰¥ 3.7
* `transformers` library

Install dependencies:

```bash
pip install transformers
```

---

## ğŸ“š References

* [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
* [DistilBERT: smaller, faster, cheaper](https://arxiv.org/abs/1910.01108)

---

## âœ¨ Author

Surya Kailash Ramesh

### ğŸƒâ€â™€ï¸ 4ï¸âƒ£ Run the script

Run your Python file from the terminal:

```bash
python text_questions.py
```

